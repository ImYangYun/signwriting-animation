# -*- coding: utf-8 -*-
"""
æ¨¡å‹å®Œæ•´è¯Šæ–­ - é€ä¸ªéƒ¨åˆ†æµ‹è¯•

åˆå¹¶äº†æ‰€æœ‰æµ‹è¯•ï¼Œä¸€æ¬¡è¿è¡Œå…¨éƒ¨æ£€æŸ¥
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

print("=" * 70)
print("æ¨¡å‹å®Œæ•´è¯Šæ–­")
print("=" * 70)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

# å¯¼å…¥ç»„ä»¶
from CAMDM.network.models import PositionalEncoding, TimestepEmbedder, MotionProcess, seq_encoder_factory

# åŸºæœ¬å‚æ•°
B = 2
K = 178
D = 3
T_future = 20
T_past = 40
latent_dim = 256

results = {}  # å­˜å‚¨æµ‹è¯•ç»“æœ

# ============================================================
print("\n" + "=" * 70)
print("Test 1: MotionProcess")
print("=" * 70)

motion_process = MotionProcess(K * D, latent_dim).to(device)

x1 = torch.randn(B, K, D, T_future).to(device) * 0.01
x2 = torch.randn(B, K, D, T_future).to(device) * 100

out1 = motion_process(x1)
out2 = motion_process(x2)

diff = (out1 - out2).abs().mean().item()
print(f"è¾“å…¥ x1 èŒƒå›´: [{x1.min():.4f}, {x1.max():.4f}]")
print(f"è¾“å…¥ x2 èŒƒå›´: [{x2.min():.4f}, {x2.max():.4f}]")
print(f"è¾“å‡ºå·®å¼‚: {diff:.4f}")

results['MotionProcess'] = 'âœ“' if diff > 0.1 else 'âš ï¸'
print(f"ç»“æœ: {results['MotionProcess']} {'å¯¹è¾“å…¥æ•æ„Ÿ' if diff > 0.1 else 'å¯¹è¾“å…¥ä¸æ•æ„Ÿ!'}")

# ============================================================
print("\n" + "=" * 70)
print("Test 2: TimestepEmbedder")
print("=" * 70)

pos_enc = PositionalEncoding(latent_dim, dropout=0.1)
timestep_emb = TimestepEmbedder(latent_dim, pos_enc).to(device)

t_0 = torch.tensor([0]).to(device)
t_7 = torch.tensor([7]).to(device)

emb_0 = timestep_emb(t_0)
emb_7 = timestep_emb(t_7)

diff = (emb_0 - emb_7).abs().mean().item()
print(f"t=0 emb: mean={emb_0.mean():.4f}, std={emb_0.std():.4f}")
print(f"t=7 emb: mean={emb_7.mean():.4f}, std={emb_7.std():.4f}")
print(f"å·®å¼‚: {diff:.4f}")

results['TimestepEmbedder'] = 'âœ“' if diff > 0.1 else 'âš ï¸'
print(f"ç»“æœ: {results['TimestepEmbedder']} {'å¯¹ t æ•æ„Ÿ' if diff > 0.1 else 'å¯¹ t ä¸æ•æ„Ÿ!'}")

# ============================================================
print("\n" + "=" * 70)
print("Test 3: ContextEncoder (MeanPool)")
print("=" * 70)

class ContextEncoder(nn.Module):
    def __init__(self, input_feats, latent_dim, num_layers=2, num_heads=4, dropout=0.1):
        super().__init__()
        self.pose_encoder = nn.Linear(input_feats, latent_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=latent_dim, nhead=num_heads,
            dim_feedforward=latent_dim * 4, dropout=dropout,
            activation="gelu", batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        if x.dim() == 4:
            B, T, J, C = x.shape
            x = x.reshape(B, T, J * C)
        x_emb = self.pose_encoder(x)
        x_enc = self.encoder(x_emb)
        return x_enc.mean(dim=1).unsqueeze(0)

context_enc = ContextEncoder(K * D, latent_dim).to(device)

past1 = torch.randn(B, T_past, K, D).to(device)
past2 = torch.randn(B, T_past, K, D).to(device) * 10

ctx1 = context_enc(past1)
ctx2 = context_enc(past2)

diff = (ctx1 - ctx2).abs().mean().item()
print(f"è¾“å‡º shape: {ctx1.shape}")
print(f"ä¸åŒè¾“å…¥å·®å¼‚: {diff:.4f}")

results['ContextEncoder'] = 'âœ“' if diff > 0.1 else 'âš ï¸'
print(f"ç»“æœ: {results['ContextEncoder']} {'å¯¹è¾“å…¥æ•æ„Ÿ' if diff > 0.1 else 'å¯¹è¾“å…¥ä¸æ•æ„Ÿ!'}")

# ============================================================
print("\n" + "=" * 70)
print("Test 4: OutputProcessMLP")
print("=" * 70)

class ResidualBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim * 2)
        self.fc2 = nn.Linear(dim * 2, dim)
        self.act = nn.GELU()
        self.ln = nn.LayerNorm(dim)

    def forward(self, x):
        residual = x
        x = self.act(self.fc1(x))
        x = self.fc2(x)
        return self.ln(x + residual * 0.5)

class OutputProcessMLP(nn.Module):
    def __init__(self, num_latent_dims, num_keypoints, num_dims_per_keypoint, hidden_dim=1024, num_layers=6):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.num_dims_per_keypoint = num_dims_per_keypoint
        self.in_proj = nn.Linear(num_latent_dims, hidden_dim)
        self.blocks = nn.ModuleList([ResidualBlock(hidden_dim) for _ in range(num_layers)])
        self.out_proj = nn.Linear(hidden_dim, num_keypoints * num_dims_per_keypoint)

    def forward(self, x):
        T, B, D = x.shape
        h = self.in_proj(x)
        for blk in self.blocks:
            h = blk(h)
        y = self.out_proj(h)
        return y.reshape(T, B, self.num_keypoints, self.num_dims_per_keypoint)

output_mlp = OutputProcessMLP(latent_dim, K, D).to(device)

# æœ‰æ—¶é—´å˜åŒ–çš„è¾“å…¥
input_motion = torch.randn(T_future, B, latent_dim).to(device)
for t in range(T_future):
    input_motion[t] += t * 0.5

out_motion = output_mlp(input_motion)
motion_disp = (out_motion[1:] - out_motion[:-1]).abs().mean().item()

# å¸¸æ•°è¾“å…¥
input_const = torch.randn(1, B, latent_dim).to(device).expand(T_future, -1, -1).clone()
out_const = output_mlp(input_const)
const_disp = (out_const[1:] - out_const[:-1]).abs().mean().item()

print(f"æœ‰æ—¶é—´å˜åŒ–è¾“å…¥ -> è¾“å‡ºå¸§é—´å·®å¼‚: {motion_disp:.6f}")
print(f"å¸¸æ•°è¾“å…¥ -> è¾“å‡ºå¸§é—´å·®å¼‚: {const_disp:.6f}")

results['OutputProcessMLP'] = 'âœ“' if motion_disp > const_disp * 1.5 else 'âš ï¸'
print(f"ç»“æœ: {results['OutputProcessMLP']} {'èƒ½ä¼ é€’æ—¶é—´å˜åŒ–' if motion_disp > const_disp * 1.5 else 'ä¸èƒ½ä¼ é€’æ—¶é—´å˜åŒ–!'}")

# ============================================================
print("\n" + "=" * 70)
print("Test 5: Transformer å¯¹ future éƒ¨åˆ†æ•æ„Ÿæ€§")
print("=" * 70)

seq_encoder = seq_encoder_factory(
    arch="trans_enc", latent_dim=latent_dim, ff_size=1024,
    num_layers=8, num_heads=4, dropout=0.2, activation="gelu"
).to(device)

seq_len = 3 + T_future  # time + sign + past_ctx + future

xseq_base = torch.randn(seq_len, B, latent_dim).to(device)
xseq_modified = xseq_base.clone()
xseq_modified[3:] = torch.randn(T_future, B, latent_dim).to(device) * 10

enc_base = seq_encoder(xseq_base)
enc_modified = seq_encoder(xseq_modified)

diff_future = (enc_base[-T_future:] - enc_modified[-T_future:]).abs().mean().item()
print(f"åªæ”¹å˜ future éƒ¨åˆ†ï¼Œè¾“å‡ºå·®å¼‚: {diff_future:.4f}")

results['Transformer_future'] = 'âœ“' if diff_future > 0.1 else 'âš ï¸'
print(f"ç»“æœ: {results['Transformer_future']} {'å¯¹ future æ•æ„Ÿ' if diff_future > 0.1 else 'å¯¹ future ä¸æ•æ„Ÿ!'}")

# ============================================================
print("\n" + "=" * 70)
print("Test 6: æ¢¯åº¦åˆ†æ - å„éƒ¨åˆ†å¯¹è¾“å‡ºçš„è´¡çŒ®")
print("=" * 70)

seq_encoder_grad = seq_encoder_factory(
    arch="trans_enc", latent_dim=latent_dim, ff_size=1024,
    num_layers=8, num_heads=4, dropout=0.0, activation="gelu"
).to(device)

xseq_grad = torch.randn(seq_len, B, latent_dim, device=device, requires_grad=True)
output = seq_encoder_grad(xseq_grad)
loss = output[-T_future:].sum()
loss.backward()

grad = xseq_grad.grad
grad_time = grad[0].abs().mean().item()
grad_sign = grad[1].abs().mean().item()
grad_past = grad[2].abs().mean().item()
grad_future = grad[3:].abs().mean().item()

total_grad = grad_time + grad_sign + grad_past + grad_future

print(f"å„éƒ¨åˆ†æ¢¯åº¦å æ¯”:")
print(f"  time: {grad_time/total_grad:.1%}")
print(f"  sign: {grad_sign/total_grad:.1%}")
print(f"  past: {grad_past/total_grad:.1%}")
print(f"  future/x_t: {grad_future/total_grad:.1%}")

results['Gradient_xt'] = 'âœ“' if grad_future/total_grad > 0.2 else 'âš ï¸'
print(f"ç»“æœ: {results['Gradient_xt']} {'x_t æ¢¯åº¦è¶³å¤Ÿ' if grad_future/total_grad > 0.2 else 'x_t æ¢¯åº¦å¤ªå°!'}")

# ============================================================
print("\n" + "=" * 70)
print("Test 7: ğŸ”¥ å®Œæ•´æ¨¡å‹ - æ˜¯å¦ä½¿ç”¨ x_t")
print("=" * 70)

try:
    from signwriting_animation.diffusion.core.models import SignWritingToPoseDiffusionV2
    
    model = SignWritingToPoseDiffusionV2(
        num_keypoints=K, num_dims_per_keypoint=D,
        residual_scale=0.1, use_mean_pool=True,
    ).to(device)
    
    model.eval()
    with torch.no_grad():
        past_fixed = torch.randn(B, K, D, T_past).to(device)
        sign_fixed = torch.randn(B, 3, 224, 224).to(device)
        t_fixed = torch.tensor([4, 4]).to(device)
        
        x_t_1 = torch.randn(B, K, D, T_future).to(device) * 0.1
        x_t_2 = torch.randn(B, K, D, T_future).to(device) * 10
        
        out_1 = model(x_t_1, t_fixed, past_fixed, sign_fixed)
        out_2 = model(x_t_2, t_fixed, past_fixed, sign_fixed)
        
        diff = (out_1 - out_2).abs().mean().item()
        
        print(f"x_t_1 èŒƒå›´: [{x_t_1.min():.2f}, {x_t_1.max():.2f}]")
        print(f"x_t_2 èŒƒå›´: [{x_t_2.min():.2f}, {x_t_2.max():.2f}]")
        print(f"è¾“å‡ºå·®å¼‚: {diff:.6f}")
        
        results['Model_uses_xt'] = 'âœ“' if diff > 0.01 else 'âš ï¸âš ï¸âš ï¸'
        print(f"ç»“æœ: {results['Model_uses_xt']} {'æ¨¡å‹ä½¿ç”¨ x_t' if diff > 0.01 else 'æ¨¡å‹å¿½ç•¥ x_t!'}")

except Exception as e:
    print(f"æµ‹è¯•å¤±è´¥: {e}")
    results['Model_uses_xt'] = 'âŒ'

# ============================================================
print("\n" + "=" * 70)
print("Test 8: ğŸ”¥ å®Œæ•´æ¨¡å‹ - æ˜¯å¦ä½¿ç”¨ timestep t")
print("=" * 70)

try:
    model.eval()
    with torch.no_grad():
        x_t_fixed = torch.randn(B, K, D, T_future).to(device)
        past_fixed = torch.randn(B, K, D, T_past).to(device)
        sign_fixed = torch.randn(B, 3, 224, 224).to(device)
        
        t_0 = torch.tensor([0, 0]).to(device)
        t_7 = torch.tensor([7, 7]).to(device)
        
        out_t0 = model(x_t_fixed, t_0, past_fixed, sign_fixed)
        out_t7 = model(x_t_fixed, t_7, past_fixed, sign_fixed)
        
        diff = (out_t0 - out_t7).abs().mean().item()
        
        print(f"t=0 vs t=7 è¾“å‡ºå·®å¼‚: {diff:.6f}")
        
        results['Model_uses_t'] = 'âœ“' if diff > 0.01 else 'âš ï¸'
        print(f"ç»“æœ: {results['Model_uses_t']} {'æ¨¡å‹ä½¿ç”¨ t' if diff > 0.01 else 'æ¨¡å‹å¿½ç•¥ t!'}")

except Exception as e:
    print(f"æµ‹è¯•å¤±è´¥: {e}")
    results['Model_uses_t'] = 'âŒ'

# ============================================================
print("\n" + "=" * 70)
print("Test 9: ğŸ”¥ å•æ ·æœ¬ Overfit (Diffusion)")
print("=" * 70)

try:
    from CAMDM.diffusion.gaussian_diffusion import GaussianDiffusion, ModelMeanType, ModelVarType, LossType
    
    def cosine_beta_schedule(timesteps, s=0.008):
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0.0001, 0.9999)
    
    DIFFUSION_STEPS = 8
    betas = cosine_beta_schedule(DIFFUSION_STEPS).numpy()
    
    diffusion = GaussianDiffusion(
        betas=betas, model_mean_type=ModelMeanType.START_X,
        model_var_type=ModelVarType.FIXED_SMALL, loss_type=LossType.MSE,
    )
    
    model = SignWritingToPoseDiffusionV2(
        num_keypoints=K, num_dims_per_keypoint=D,
        residual_scale=0.1, use_mean_pool=True,
    ).to(device)
    
    # GT æœ‰æ˜æ˜¾è¿åŠ¨
    gt = torch.zeros(1, K, D, T_future).to(device)
    for t_idx in range(T_future):
        gt[:, :, 0, t_idx] = t_idx * 0.5
    
    gt_disp = (gt[:, :, :, 1:] - gt[:, :, :, :-1]).abs().mean().item()
    print(f"GT displacement: {gt_disp:.4f}")
    
    past = torch.zeros(1, K, D, T_past).to(device)
    sign = torch.randn(1, 3, 224, 224).to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    
    print("\nDiffusion è®­ç»ƒ 1500 æ­¥...")
    model.train()
    for step in range(1500):
        optimizer.zero_grad()
        
        t = torch.randint(0, DIFFUSION_STEPS, (1,), device=device)
        noise = torch.randn_like(gt)
        x_t = diffusion.q_sample(gt, t, noise=noise)
        
        pred = model(x_t, t, past, sign)
        loss = F.mse_loss(pred, gt)
        loss.backward()
        optimizer.step()
        
        if step % 300 == 0:
            pred_disp = (pred[:, :, :, 1:] - pred[:, :, :, :-1]).abs().mean().item()
            ratio = pred_disp / gt_disp
            print(f"  Step {step}: loss={loss.item():.6f}, pred_disp={pred_disp:.4f}, ratio={ratio:.4f}")
    
    # æµ‹è¯•
    model.eval()
    with torch.no_grad():
        t = torch.tensor([0]).to(device)
        x_t = diffusion.q_sample(gt, t, noise=torch.randn_like(gt))
        pred = model(x_t, t, past, sign)
        
        pred_disp = (pred[:, :, :, 1:] - pred[:, :, :, :-1]).abs().mean().item()
        diffusion_ratio = pred_disp / gt_disp
        
        print(f"\nDiffusion æœ€ç»ˆ: pred_disp={pred_disp:.4f}, ratio={diffusion_ratio:.4f}")
        
        results['Diffusion_overfit'] = 'âœ“' if diffusion_ratio > 0.5 else 'âš ï¸'

except Exception as e:
    print(f"æµ‹è¯•å¤±è´¥: {e}")
    diffusion_ratio = 0
    results['Diffusion_overfit'] = 'âŒ'

# ============================================================
print("\n" + "=" * 70)
print("Test 10: ğŸ”¥ å•æ ·æœ¬ Overfit (Regression å¯¹æ¯”)")
print("=" * 70)

try:
    model_reg = SignWritingToPoseDiffusionV2(
        num_keypoints=K, num_dims_per_keypoint=D,
        residual_scale=0.1, use_mean_pool=True,
    ).to(device)
    
    optimizer = torch.optim.AdamW(model_reg.parameters(), lr=1e-3)
    
    print("Regression è®­ç»ƒ 1500 æ­¥ (ä¸ç”¨ Diffusion)...")
    model_reg.train()
    
    for step in range(1500):
        optimizer.zero_grad()
        
        t = torch.tensor([0]).to(device)  # å›ºå®š t=0
        x_t = torch.zeros(1, K, D, T_future).to(device)  # é›¶è¾“å…¥
        
        pred = model_reg(x_t, t, past, sign)
        
        loss_mse = F.mse_loss(pred, gt)
        pred_vel = pred[:, :, :, 1:] - pred[:, :, :, :-1]
        gt_vel = gt[:, :, :, 1:] - gt[:, :, :, :-1]
        loss_vel = F.mse_loss(pred_vel, gt_vel)
        
        loss = loss_mse + loss_vel
        loss.backward()
        optimizer.step()
        
        if step % 300 == 0:
            pred_disp = (pred[:, :, :, 1:] - pred[:, :, :, :-1]).abs().mean().item()
            ratio = pred_disp / gt_disp
            print(f"  Step {step}: loss={loss.item():.6f}, pred_disp={pred_disp:.4f}, ratio={ratio:.4f}")
    
    # æµ‹è¯•
    model_reg.eval()
    with torch.no_grad():
        pred = model_reg(x_t, t, past, sign)
        pred_disp = (pred[:, :, :, 1:] - pred[:, :, :, :-1]).abs().mean().item()
        regression_ratio = pred_disp / gt_disp
        
        print(f"\nRegression æœ€ç»ˆ: pred_disp={pred_disp:.4f}, ratio={regression_ratio:.4f}")
        
        results['Regression_overfit'] = 'âœ“' if regression_ratio > 0.5 else 'âš ï¸'

except Exception as e:
    print(f"æµ‹è¯•å¤±è´¥: {e}")
    regression_ratio = 0
    results['Regression_overfit'] = 'âŒ'

# ============================================================
print("\n" + "=" * 70)
print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
print("=" * 70)

print("\nç»„ä»¶æµ‹è¯•:")
for key in ['MotionProcess', 'TimestepEmbedder', 'ContextEncoder', 'OutputProcessMLP', 'Transformer_future', 'Gradient_xt']:
    if key in results:
        print(f"  {key}: {results[key]}")

print("\nå®Œæ•´æ¨¡å‹æµ‹è¯•:")
for key in ['Model_uses_xt', 'Model_uses_t']:
    if key in results:
        print(f"  {key}: {results[key]}")

print("\nOverfit æµ‹è¯•:")
print(f"  Diffusion: {results.get('Diffusion_overfit', '?')} (ratio={diffusion_ratio:.4f})")
print(f"  Regression: {results.get('Regression_overfit', '?')} (ratio={regression_ratio:.4f})")

# ============================================================
print("\n" + "=" * 70)
print("ğŸ” è¯Šæ–­ç»“è®º")
print("=" * 70)

if results.get('Model_uses_xt') == 'âš ï¸âš ï¸âš ï¸':
    print("""
âš ï¸ æ¨¡å‹å¿½ç•¥äº† x_t è¾“å…¥!

å¯èƒ½åŸå› :
1. Transformer å­¦ä¼šäº†ä¸»è¦ä¾èµ–æ¡ä»¶ tokens (time, sign, past)
2. x_t é€šè¿‡ MotionProcess åä¿¡æ¯è¢«å‹ç¼©
3. æ¨¡å‹å‘ç°å¿½ç•¥ x_t ä¹Ÿèƒ½æœ€å°åŒ– MSE loss
""")

if results.get('Diffusion_overfit') == 'âš ï¸' and results.get('Regression_overfit') == 'âœ“':
    print("""
âš ï¸ Regression æˆåŠŸä½† Diffusion å¤±è´¥!

è¯´æ˜æ¨¡å‹æ¶æ„æœ¬èº«æ²¡é—®é¢˜ï¼Œé—®é¢˜åœ¨ Diffusion çš„ä½¿ç”¨æ–¹å¼:
1. MSE loss è®©æ¨¡å‹æ‰¾åˆ°äº† "è¾“å‡ºå‡å€¼" çš„æ·å¾„
2. éœ€è¦é¢å¤–çš„ loss (velocity, displacement) å¼ºåˆ¶å­¦ä¹ è¿åŠ¨
3. æˆ–è€…æ¢ç”¨ EPSILON mode
""")

if results.get('Regression_overfit') == 'âš ï¸':
    print("""
âš ï¸ Regression ä¹Ÿå¤±è´¥äº†!

è¯´æ˜æ¨¡å‹æ¶æ„æœ¬èº«å¯èƒ½æœ‰é—®é¢˜:
1. æ£€æŸ¥ OutputProcessMLP æ˜¯å¦æ­£ç¡®ä¼ é€’æ—¶é—´ä¿¡æ¯
2. æ£€æŸ¥å„ç»„ä»¶çš„è¿æ¥æ–¹å¼
3. æ£€æŸ¥æ¢¯åº¦æµæ˜¯å¦é€šç•…
""")

print("\n" + "=" * 70)