# -*- coding: utf-8 -*-
import os
import torch
import numpy as np
import lightning as pl
from torch.utils.data import DataLoader

from pose_format import Pose
from pose_format.numpy.pose_body import NumPyPoseBody
from pose_format.utils.generic import reduce_holistic
from pose_format.torch.masked.collator import zero_pad_collator

from signwriting_animation.data.data_loader import DynamicPosePredictionDataset
from signwriting_animation.diffusion.lightning_module import LitMinimal, sanitize_btjc, masked_dtw

try:
    from pose_anonymization.data.normalization import unshift_hands
    HAS_UNSHIFT = True
    print("[âœ“] Successfully imported unshift_hands")
except ImportError as e:
    print(f"[âœ—] Warning: Could not import unshift_hands: {e}")
    print("[!] PRED poses will have incorrect hand positions!")
    HAS_UNSHIFT = False


def tensor_to_pose_complete(
    t_btjc: torch.Tensor,
    header,
    ref_pose: Pose,
    apply_unshift: bool = True,
    match_scale_to_ref: bool = True,
):
    """
    å®Œæ•´çš„ tensor â†’ pose è½¬æ¢ï¼š
    1. æ­£ç¡®çš„ confidence shape (3D)
    2. ä½¿ç”¨ GT çš„ FPS å’Œ confidence
    3. å¯é€‰åœ°è°ƒç”¨ unshift_hands
    4. å¯é€‰åœ°æ ¹æ® ref_pose çš„ç©ºé—´æ–¹å·®å¯¹ PRED åšæ•´ä½“ç¼©æ”¾
    5. å†æŠŠ PRED çš„å…¨å±€ä¸­å¿ƒå¹³ç§»åˆ°å’Œ ref_pose å¯¹é½ï¼ˆåªå½±å“å¯è§†åŒ–ï¼‰
    """
    if t_btjc.dim() == 4:
        t = t_btjc[0]
    elif t_btjc.dim() == 3:
        t = t_btjc
    else:
        raise ValueError(f"Expected 3D or 4D tensor, got {t_btjc.dim()}D")

    t_np = t.detach().cpu().numpy().astype(np.float32)  # [T,J,C]

    print(f"\n[tensor_to_pose_complete] (apply_unshift={apply_unshift}, match_scale_to_ref={match_scale_to_ref})")
    print(f"  è¾“å…¥ shape: {t_np.shape}")
    print(f"  è¾“å…¥ range: [{t_np.min():.4f}, {t_np.max():.4f}]")

    # ---- å…ˆåˆ›å»º Pose å¯¹è±¡ï¼ˆè¿˜ä¸ç¼©æ”¾ / ä¸å¹³ç§»ï¼‰----
    arr = t_np[:, None, :, :]  # [T, 1, J, C]
    conf = ref_pose.body.confidence[:len(t_np)].copy()
    fps = ref_pose.body.fps

    body = NumPyPoseBody(fps=fps, data=arr, confidence=conf)
    pose_obj = Pose(header=header, body=body)

    print(f"  åˆ›å»º Pose:")
    print(f"    fps: {fps}")
    print(f"    data shape: {pose_obj.body.data.shape}")
    print(f"    conf shape: {pose_obj.body.confidence.shape}")
    print(f"    data range: [{pose_obj.body.data.min():.4f}, {pose_obj.body.data.max():.4f}]")

    if apply_unshift and HAS_UNSHIFT:
        print(f"\n  è°ƒç”¨ unshift_hands...")
        try:
            unshift_hands(pose_obj)
            print(f"    âœ“ unshift æˆåŠŸ")
            print(f"    new range: [{pose_obj.body.data.min():.4f}, {pose_obj.body.data.max():.4f}]")
        except Exception as e:
            print(f"    âœ— unshift å¤±è´¥: {e}")
    elif apply_unshift and not HAS_UNSHIFT:
        print(f"\n  âš ï¸  é¢„æœŸ unshift_hands ä½†æœªå¯¼å…¥æˆåŠŸ")
    else:
        print(f"\n  âš ï¸  æœ¬æ¬¡ä¸è°ƒç”¨ unshift_handsï¼Œä»…å†™å…¥ raw åæ ‡")

    if match_scale_to_ref:
        try:
            T_pred = pose_obj.body.data.shape[0]
            ref_arr = np.asarray(ref_pose.body.data[:T_pred, 0], dtype=np.float32)   # [T,J,C]
            pred_arr = np.asarray(pose_obj.body.data[:T_pred, 0], dtype=np.float32)  # [T,J,C]

            def _var_tjc(a):
                center = a.mean(axis=1, keepdims=True)
                return float(((a - center) ** 2).mean())

            var_ref = _var_tjc(ref_arr)
            var_pred = _var_tjc(pred_arr)

            print(f"\n  [scale] ref_var={var_ref:.4f}, pred_var={var_pred:.4f}")
            if var_pred > 1e-8 and var_ref > 0:
                scale = float(np.sqrt((var_ref + 1e-6) / (var_pred + 1e-6)))
                print(f"  [scale] apply scale={scale:.3f}")
                pose_obj.body.data *= scale
                pred_arr = np.asarray(pose_obj.body.data[:T_pred, 0], dtype=np.float32)
                print(f"  scaled data range: [{pose_obj.body.data.min():.4f}, {pose_obj.body.data.max():.4f}]")
            else:
                print("  [scale] var too small, skip scale")
        except Exception as e:
            print(f"  [scale] è®¡ç®—ç¼©æ”¾ç³»æ•°å¤±è´¥ï¼Œè·³è¿‡ç¼©æ”¾: {e}")
            T_pred = pose_obj.body.data.shape[0]
            ref_arr = np.asarray(ref_pose.body.data[:T_pred, 0], dtype=np.float32)
            pred_arr = np.asarray(pose_obj.body.data[:T_pred, 0], dtype=np.float32)
    else:
        T_pred = pose_obj.body.data.shape[0]
        ref_arr = np.asarray(ref_pose.body.data[:T_pred, 0], dtype=np.float32)
        pred_arr = np.asarray(pose_obj.body.data[:T_pred, 0], dtype=np.float32)

    try:
        ref_center = ref_arr.reshape(-1, 3).mean(axis=0)   # [3]
        pred_center = pred_arr.reshape(-1, 3).mean(axis=0) # [3]
        delta = ref_center - pred_center                   # [3]
        print(f"\n  [translate] ref_center={ref_center}, pred_center={pred_center}")
        print(f"  [translate] apply delta={delta}")
        pose_obj.body.data += delta  # broadcast åˆ° [T,1,J,C]
        print(f"  translated data range: [{pose_obj.body.data.min():.4f}, {pose_obj.body.data.max():.4f}]")
    except Exception as e:
        print(f"  [translate] å¹³ç§»å¯¹é½å¤±è´¥ï¼Œè·³è¿‡å¹³ç§»: {e}")

    return pose_obj


# ---------------------------------------------------------------------
# è¯»å› .pose æ–‡ä»¶ï¼Œåšç®€å•æ•°å€¼æ£€æŸ¥
# ---------------------------------------------------------------------
def inspect_pose(path: str, name: str):
    """
    è¯»å› .pose æ–‡ä»¶ï¼Œæ‰“å°:
    - æ•°æ® shape
    - å…¨å±€æœ€å°/æœ€å¤§å€¼
    - æ¯å¸§éª¨æ¶çš„å¹³å‡æ–¹å·®
    """
    if not os.path.exists(path):
        print(f"\n[{name}] æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return

    with open(path, "rb") as f:
        pose = Pose.read(f)

    data = pose.body.data  # [T, P, J, C]
    data_np = np.asarray(data, dtype=np.float32)
    T, P, J, C = data_np.shape
    data_tjc = data_np.reshape(T, P * J, C)

    center = data_tjc.mean(axis=1, keepdims=True)  # [T,1,C]
    var = ((data_tjc - center) ** 2).mean(axis=(1, 2))  # [T]

    print(f"\n[{name}] {path}")
    print(f"  shape: {data_np.shape}")
    print(f"  range: [{data_np.min():.4f}, {data_np.max():.4f}]")
    print(f"  per-frame var min/max: [{var.min():.6f}, {var.max():.6f}]")


if __name__ == "__main__":
    pl.seed_everything(42)

    data_dir = "/home/yayun/data/pose_data/"
    csv_path = "/home/yayun/data/signwriting-animation/data_fixed.csv"
    out_dir = "logs/minimal_178_aligned"
    os.makedirs(out_dir, exist_ok=True)

    stats_path = f"{data_dir}/mean_std_178_with_preprocess.pt"

    print("\n" + "="*70)
    print("å®Œå…¨å¯¹é½ç‰ˆæœ¬ + å¯è§†åŒ–ç¼©æ”¾ & å¹³ç§»ä¿®æ­£")
    print("="*70)
    print("  âœ… LightningModule.unnormalize: åªåšæ•°å€¼åå½’ä¸€åŒ–")
    print("  âœ… tensor_to_pose: è°ƒç”¨ unshift_handsï¼ˆè‹¥å¯ç”¨ï¼‰")
    print("  âœ… å¯è§†åŒ–æ—¶æ ¹æ® ref_pose æ–¹å·® + ä¸­å¿ƒå¯¹ PRED åš scale & translate")
    print("="*70 + "\n")

    # Datasetï¼šåªå–ä¸€ä¸ªæ ·æœ¬åš overfit
    base_ds = DynamicPosePredictionDataset(
        data_dir=data_dir,
        csv_path=csv_path,
        num_past_frames=40,
        num_future_frames=20,
        with_metadata=True,
        split="train",
    )

    sample_0 = base_ds[0]

    class FixedSampleDataset(torch.utils.data.Dataset):
        def __init__(self, sample):
            self.sample = sample
        def __len__(self):
            return 1
        def __getitem__(self, idx):
            return self.sample

    train_ds = FixedSampleDataset(sample_0)
    train_loader = DataLoader(
        train_ds, batch_size=1, shuffle=False, collate_fn=zero_pad_collator
    )

    trainer = pl.Trainer(
        max_epochs=100,       # ğŸ‘‰ æƒ³æ›´å¼º overfit å¯ä»¥æ”¹æˆ 500 / 1000
        accelerator="gpu",
        devices=1,
        enable_checkpointing=False,
        log_every_n_steps=50,
    )

    num_joints = sample_0["data"].shape[-2]
    num_dims = sample_0["data"].shape[-1]

    model = LitMinimal(
        num_keypoints=num_joints,
        num_dims=num_dims,
        stats_path=stats_path,
        lr=1e-3,
        diffusion_steps=50,
        beta_start=1e-4,
        beta_end=2e-2,
        pred_target="x0",
    )

    print("\n[è®­ç»ƒä¸­...]")
    trainer.fit(model, train_loader)

    print("\n" + "="*70)
    print("INFERENCE")
    print("="*70)

    model.eval()
    device = trainer.strategy.root_device
    model = model.to(device)

    with torch.no_grad():
        batch = next(iter(train_loader))
        cond = batch["conditions"]

        past = sanitize_btjc(cond["input_pose"][:1]).to(device)
        sign = cond["sign_image"][:1].float().to(device)
        gt = sanitize_btjc(batch["data"][:1]).to(device)

        future_len = gt.size(1)

        print(f"\n[é‡‡æ ·] diffusion_steps=50, future_len={future_len}")

        pred = model.sample_autoregressive_fast(
            past_btjc=past,
            sign_img=sign,
            future_len=future_len,
            chunk=20,
        )

        print(f"\nGT (raw):   [{gt.min():.4f}, {gt.max():.4f}]")
        print(f"PRED (raw): [{pred.min():.4f}, {pred.max():.4f}]")

        mask_bt = torch.ones(1, future_len, device=device)
        dtw_val = masked_dtw(pred, gt, mask_bt)
        print(f"DTW: {dtw_val:.4f}")

        # å¯é€‰ï¼šæ£€æŸ¥å¸§é—´å¹³å‡ä½ç§»ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯â€œå‡ ä¹é™æ­¢â€
        pred_np = pred.cpu().numpy()
        disp = np.linalg.norm(pred_np[:, 1:] - pred_np[:, :-1], axis=-1).mean()
        print(f"mean frame-to-frame displacement: {disp:.6f}")

    # -----------------------------------------------------------------
    # åŠ è½½åŸå§‹å‚è€ƒ poseï¼ˆå¤§åæ ‡ç³»ï¼‰ï¼Œå¹¶ä¿å­˜ GT / PRED
    # -----------------------------------------------------------------
    print("\n" + "="*70)
    print("åŠ è½½å‚è€ƒ pose")
    print("="*70)

    ref_path = base_ds.records[0]["pose"]
    ref_path = ref_path if os.path.isabs(ref_path) else os.path.join(data_dir, ref_path)

    with open(ref_path, "rb") as f:
        ref_pose = Pose.read(f)

    ref_pose = reduce_holistic(ref_pose)
    ref_pose = ref_pose.remove_components(["POSE_WORLD_LANDMARKS"])

    header = ref_pose.header

    # ä¿å­˜ GTï¼ˆåŸå§‹å‚è€ƒï¼‰
    out_gt = os.path.join(out_dir, "gt_reference.pose")
    with open(out_gt, "wb") as f:
        ref_pose.write(f)
    print(f"\nâœ“ GT (å‚è€ƒ) ä¿å­˜: {out_gt}")

    # ä¿å­˜ PREDï¼ˆå®Œæ•´æµç¨‹ï¼šunshift + scale + translateï¼‰
    print("\n" + "="*70)
    print("ä¿å­˜ PREDï¼ˆunshift + scale + translateï¼‰")
    print("="*70)

    pose_pred = tensor_to_pose_complete(
        pred, header, ref_pose, apply_unshift=True, match_scale_to_ref=True
    )
    out_pred = os.path.join(out_dir, "pred_complete.pose")
    with open(out_pred, "wb") as f:
        pose_pred.write(f)
    print(f"\nâœ“ PRED ä¿å­˜: {out_pred}")

    # è¯»å›ä¸¤ä¸ª pose åšæ•°å€¼æ£€æŸ¥
    print("\n" + "="*70)
    print("DEBUG: è¯»å› .pose æ–‡ä»¶æ£€æŸ¥åˆ†å¸ƒ")
    print("="*70)

    inspect_pose(out_gt, "GT")
    inspect_pose(out_pred, "PRED")

    # æ€»ç»“
    print("\n" + "="*70)
    print("âœ“ å®Œæˆï¼")
    print("="*70)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. GT (å‚è€ƒ): {out_gt}")
    print(f"  2. PRED:      {out_pred}")

    print(f"\nåœ¨ sign.mt ä¸­æµ‹è¯•:")
    print(f"  1. æ‰“å¼€ gt_reference.pose")
    print(f"  2. æ‰“å¼€ pred_complete.pose")

    if not HAS_UNSHIFT:
        print(f"\nâš ï¸  è­¦å‘Š:")
        print(f"  unshift_hands æœªæˆåŠŸå¯¼å…¥")
        print(f"  PRED çš„æ‰‹éƒ¨ä½ç½®å¯èƒ½ä¸æ­£ç¡®")
        print(f"  è¯·ç¡®ä¿ pose_anonymization åŒ…å¯ç”¨")
