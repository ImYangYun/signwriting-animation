# -*- coding: utf-8 -*-
"""
è¯Šæ–­ train() vs eval() å·®å¼‚

ç°è±¡ï¼š
- model.train(): ratio â‰ˆ 0.3-0.4
- model.eval(): ratio = 0.0000

å¯èƒ½åŸå› ï¼šDropout åœ¨è®­ç»ƒæ—¶å¼•å…¥éšæœºæ€§ï¼Œeval æ—¶å…³é—­å¯¼è‡´åç¼©
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

print("=" * 70)
print("è¯Šæ–­ train() vs eval() å·®å¼‚")
print("=" * 70)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

from CAMDM.network.models import PositionalEncoding, TimestepEmbedder, MotionProcess, seq_encoder_factory
from CAMDM.diffusion.gaussian_diffusion import GaussianDiffusion, ModelMeanType, ModelVarType, LossType
from transformers import CLIPModel

# ============================================================
# æ¨¡å‹å®šä¹‰ï¼ˆå®Œå…¨æ—  Dropout ç‰ˆæœ¬ï¼‰
# ============================================================

class ContextEncoderNoDropout(nn.Module):
    def __init__(self, input_feats, latent_dim, num_layers=2, num_heads=4):
        super().__init__()
        self.pose_encoder = nn.Linear(input_feats, latent_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=latent_dim, nhead=num_heads,
            dim_feedforward=latent_dim * 4,
            dropout=0.0,  # æ—  Dropout!
            activation="gelu", batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        if x.dim() == 3:
            x = x.permute(1, 0, 2)
        elif x.dim() == 4:
            B, T, J, C = x.shape
            x = x.reshape(B, T, J * C)
        x_emb = self.pose_encoder(x)
        x_enc = self.encoder(x_emb)
        context = x_enc.mean(dim=1)
        return context.unsqueeze(0)


class OutputProcessMLP(nn.Module):
    def __init__(self, num_latent_dims, num_keypoints, num_dims_per_keypoint, hidden_dim=512):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.num_dims_per_keypoint = num_dims_per_keypoint
        self.net = nn.Sequential(
            nn.Linear(num_latent_dims, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, 256),
            nn.GELU(),
            nn.Linear(256, num_keypoints * num_dims_per_keypoint),
        )

    def forward(self, x):
        T, B, D = x.shape
        y = self.net(x)
        return y.reshape(T, B, self.num_keypoints, self.num_dims_per_keypoint)


class PositionalEncodingNoDropout(nn.Module):
    """æ—  Dropout çš„ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(1)  # [max_len, 1, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [T, B, D]
        return x + self.pe[:x.size(0)]


class EmbedSignWriting(nn.Module):
    def __init__(self, num_latent_dims, embedding_arch='openai/clip-vit-base-patch32'):
        super().__init__()
        self.model = CLIPModel.from_pretrained(embedding_arch)
        self.proj = None
        if (num_embedding_dims := self.model.visual_projection.out_features) != num_latent_dims:
            self.proj = nn.Linear(num_embedding_dims, num_latent_dims)

    def forward(self, image_batch):
        embeddings_batch = self.model.get_image_features(pixel_values=image_batch)
        if self.proj is not None:
            embeddings_batch = self.proj(embeddings_batch)
        return embeddings_batch[None, ...]


class FixedModelNoDropout(nn.Module):
    """å®Œå…¨æ—  Dropout çš„æ¨¡å‹"""
    def __init__(self, num_keypoints, num_dims_per_keypoint, num_latent_dims=256,
                 ff_size=1024, num_layers=8, num_heads=4):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.num_dims_per_keypoint = num_dims_per_keypoint

        input_feats = num_keypoints * num_dims_per_keypoint
        
        self.future_motion_process = MotionProcess(input_feats, num_latent_dims)
        self.past_motion_process = MotionProcess(input_feats, num_latent_dims)
        
        # æ—  Dropout çš„ä½ç½®ç¼–ç 
        self.sequence_pos_encoder = PositionalEncodingNoDropout(num_latent_dims)

        self.embed_signwriting = EmbedSignWriting(num_latent_dims)
        
        # TimestepEmbedder ç”¨åŸç‰ˆï¼ˆå®ƒå†…éƒ¨ä¹Ÿç”¨ PositionalEncodingï¼‰
        # æˆ‘ä»¬ç”¨ç®€å•çš„ embedding æ›¿ä»£
        self.embed_timestep = nn.Embedding(1000, num_latent_dims)

        # æ—  Dropout çš„ ContextEncoder
        self.past_context_encoder = ContextEncoderNoDropout(input_feats, num_latent_dims)
        print(f"âœ“ ä½¿ç”¨ MeanPool æ¨¡å¼ (æ—  Dropout)")

        # æ—  Dropout çš„ Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=num_latent_dims, nhead=num_heads,
            dim_feedforward=ff_size,
            dropout=0.0,  # æ—  Dropout!
            activation="gelu", batch_first=False,
        )
        self.seqEncoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        print(f"âœ“ Transformer (æ—  Dropout)")

        self.pose_projection = OutputProcessMLP(
            num_latent_dims, num_keypoints, num_dims_per_keypoint
        )

        self.future_time_proj = nn.Sequential(
            nn.Linear(1, num_latent_dims),
            nn.SiLU(),
            nn.Linear(num_latent_dims, num_latent_dims)
        )

    def forward(self, x, timesteps, past_motion, signwriting_im_batch):
        batch_size, num_keypoints, num_dims_per_keypoint, num_frames = x.shape

        if past_motion.dim() == 4:
            if past_motion.shape[1] == num_keypoints and past_motion.shape[2] == num_dims_per_keypoint:
                pass
            elif past_motion.shape[2] == num_keypoints and past_motion.shape[3] == num_dims_per_keypoint:
                past_motion = past_motion.permute(0, 2, 3, 1).contiguous()

        T_past = past_motion.shape[-1]
        T_future = num_frames
        B = batch_size

        # Timestep embedding (ç®€åŒ–ç‰ˆ)
        time_emb = self.embed_timestep(timesteps.clamp(0, 999)).unsqueeze(0)  # [1, B, D]
        
        signwriting_emb = self.embed_signwriting(signwriting_im_batch)
        future_motion_emb = self.future_motion_process(x)

        t = torch.linspace(0, 1, steps=T_future, device=x.device).view(T_future, 1, 1)
        t_latent = self.future_time_proj(t).expand(-1, B, -1)
        future_motion_emb = future_motion_emb + 0.1 * t_latent

        past_btjc = past_motion.permute(0, 3, 1, 2).contiguous()
        past_context = self.past_context_encoder(past_btjc)
        xseq = torch.cat([time_emb, signwriting_emb, past_context, future_motion_emb], dim=0)
        
        xseq = self.sequence_pos_encoder(xseq)
        output = self.seqEncoder(xseq)
        output = output[-T_future:]
        
        result = self.pose_projection(output)
        result = result.permute(1, 2, 3, 0).contiguous()

        return result


# ============================================================
# æµ‹è¯•
# ============================================================
K = 178
D = 3
T_past = 40
T_future = 20

gt_bjct = torch.zeros(1, K, D, T_future).to(device)
for t_idx in range(T_future):
    gt_bjct[:, :, 0, t_idx] = t_idx * 0.5

gt_disp = (gt_bjct[:, :, :, 1:] - gt_bjct[:, :, :, :-1]).abs().mean().item()
print(f"\nGT displacement: {gt_disp:.4f}")

past_bjct = torch.zeros(1, K, D, T_past).to(device)
for t_idx in range(T_past):
    past_bjct[:, :, 0, t_idx] = (t_idx - T_past) * 0.5

sign_img = torch.randn(1, 3, 224, 224).to(device)

# ============================================================
print("\n" + "=" * 70)
print("æµ‹è¯•: æ—  Dropout æ¨¡å‹")
print("=" * 70)

model = FixedModelNoDropout(
    num_keypoints=K,
    num_dims_per_keypoint=D,
).to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

print("\nè®­ç»ƒ...")
model.train()
for step in range(2001):
    optimizer.zero_grad()
    
    t = torch.tensor([0]).to(device)
    pred = model(gt_bjct, t, past_bjct, sign_img)
    
    loss_mse = F.mse_loss(pred, gt_bjct)
    pred_vel = pred[:, :, :, 1:] - pred[:, :, :, :-1]
    gt_vel = gt_bjct[:, :, :, 1:] - gt_bjct[:, :, :, :-1]
    loss_vel = F.mse_loss(pred_vel, gt_vel)
    
    loss = loss_mse + loss_vel
    loss.backward()
    optimizer.step()
    
    if step % 400 == 0:
        pred_disp = (pred[:, :, :, 1:] - pred[:, :, :, :-1]).abs().mean().item()
        ratio = pred_disp / gt_disp
        print(f"  Step {step}: loss={loss.item():.4f}, ratio={ratio:.4f}")

# æµ‹è¯• train æ¨¡å¼
print("\næµ‹è¯• (train æ¨¡å¼):")
model.train()
with torch.no_grad():
    t = torch.tensor([0]).to(device)
    pred = model(gt_bjct, t, past_bjct, sign_img)
    pred_disp = (pred[:, :, :, 1:] - pred[:, :, :, :-1]).abs().mean().item()
    ratio_train = pred_disp / gt_disp
    print(f"  ratio (train mode): {ratio_train:.4f}")

# æµ‹è¯• eval æ¨¡å¼
print("\næµ‹è¯• (eval æ¨¡å¼):")
model.eval()
with torch.no_grad():
    t = torch.tensor([0]).to(device)
    pred = model(gt_bjct, t, past_bjct, sign_img)
    pred_disp = (pred[:, :, :, 1:] - pred[:, :, :, :-1]).abs().mean().item()
    ratio_eval = pred_disp / gt_disp
    print(f"  ratio (eval mode): {ratio_eval:.4f}")

print("\n" + "=" * 70)
print("ğŸ“Š ç»“è®º")
print("=" * 70)

if abs(ratio_train - ratio_eval) < 0.1:
    print(f"âœ… train å’Œ eval ä¸€è‡´: train={ratio_train:.4f}, eval={ratio_eval:.4f}")
    if ratio_eval > 0.5:
        print("ğŸ‰ ä¿®å¤æˆåŠŸï¼")
    else:
        print("âš ï¸ è¿˜æœ‰å…¶ä»–é—®é¢˜å¯¼è‡´è¿åŠ¨ä¸¢å¤±")
else:
    print(f"âŒ train å’Œ eval ä¸ä¸€è‡´: train={ratio_train:.4f}, eval={ratio_eval:.4f}")
    print("   é—®é¢˜ç¡®è®¤æ˜¯ Dropout å¯¼è‡´çš„ï¼")